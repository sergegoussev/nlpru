{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Categorization with converstation thread effects\n",
    "\n",
    "When detecting topics in a document, common ways include simple *keyword* matching, *topic modeling*, and many others. While this works fine for large text documents like news articles, applying this type of approach to social media data has a serious *methodological* flaw: posts are not isolated but usually part of a conversation thread. Hence if one post is detected as being on the topic, it is logical that another post that replies to it is also on the topic, however this second post might not have used *keywords* that resulted in being detected and tagged as on the topic. \n",
    "\n",
    "This notebook walks through the problem and proposes a basic model using Twitter data. It shows how conversation thread affects can be used with an existing list of *pre-categorized* tweets to expand the number of tweets that are coded as being on the topic."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem space \n",
    "\n",
    "On Twitter, we can consider that there are 3 potential scenarios for tweets in the context of a convesation thread:\n",
    "![Conversation_thread_visual.PNG](Conversation_thread_visual.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Topics flow down in threads, not up**: the first scenario is quite simple, a tweet replies to another tweet. So for instance, if **tweet 1** is categorized as being on a certain topic, then logically every replying tweet is also on the topic (**tweets 2-5**). \n",
    "    * If **tweet 3** is on the topic, then **tweet 4** is as well, but not the others (**1, 2, or 5**)\n",
    "    * *NOTE*: This is obviously a bit of a simplification and depends on how a topic is defined. A reply can be on a separate topic, especially if the topics being analyzed are quite close logically, then the transition is harder to determine. More on this later...\n",
    "   \n",
    "2. **How Twitter stores replies for retweeted tweets impacts how we categorize RTs**: Twitter stores a reply relationship in the original (i.e. retweeted) tweet, not the tweet that retweeted it. As we often investigate tweet topics by also including the tweets that retweeted others as copies of the original, we need to take this into account.\n",
    "    * In other words, say we want to create a picture of the topics that were discussed during a particular day. We would pull all the tweets and then pull all the retweets that were made during that day, and plot them by topic per hour. This means that we pull the `retweet tweet id`, `retweet created at`, and `retweeted tweet text`. In this case, we need to check if the retweeted tweet was a reply to another tweet that was determined to be on a specific topic!\n",
    "    \n",
    "3. **Adding text/comment while retweeting also has to be taken into account**: if a user retweets a tweet, they have the option to *Retweet with comment* --which is stored by Twitter as a separate tweet but is linked (and displayed) with the retweeted tweet as embedded below. In the Twitter API, the retweeted tweet is called a `quoted_status`. Hence if a *quoted tweet*, for instance **tweet 10** is categorized on the topic, then the *commenting tweet* or **tweet 9**, must also be on the topic\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such we have 4 inputs:\n",
    "* a list of tweets with their ids and topics: `list_tweets = [('twtid','topic'),..]`\n",
    "* a list of tweets that reply to other tweets: `list_replies = [('twtid','inreplytotwtid'),...]`\n",
    "* a list of retweeted tweet ids that retweeted a previous tweet: `list_rts = [('twtid','rttwtid'),...]`; and\n",
    "* a list of quotes, i.e. when one tweet quoted another: `list_quotes = [('twtid','qttwtid'),...]`\n",
    "\n",
    "The output should be a final list of tweets with the topics changed: `recategorized_tweet_list = [('twtid','topic'),..]`\n",
    "\n",
    "*NOTE*, for simplicity, the rest of the walkthorugh will consider only 2 topics, or whether the tweet is on a specific topic or not.\n",
    "\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, there are **two ways** to solve the problem:\n",
    "1. Using the tweets that **are categorized** about the topic\n",
    "    * In other words, for each tweet that is a about a topic, use the conversation thread linkages based on certain rules to tag all related or **downstream** tweets as also about the topic. This will mean that a conversation thread is *virtually* created for each tweet coded about the topic.\n",
    "2. The reverse, starting with the tweets **not categorized** as about the topic\n",
    "    * This approach will require an iteration through all uncategorized tweets, checking each that the rules and conversation thread linkages allow the tweet to be recategorized as about the topic. This step is repeated again and again until tweets are **no longer** being recategorized. \n",
    "    \n",
    "While a thorough test of efficiency is required, option 1 requires building a conversation thread *object* first, and using it to categorize tweets. As a method does not exist, this will be done in a later section. *(Note, some like @fionapigott have created [conversation thread builders](https://github.com/fionapigott/conversation-builder), but they only work with *replies*, and not *quotes*, as there is often an interelation of quotes that starts separate conversation threads)*\n",
    "\n",
    "This workbook will thus follow the 2nd option in solving the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be based on the following steps/rules:\n",
    "1. Create a dictionary for each convesation relationship, such as `replies_dict = {'twtid':'inreplytotwtid',...}`\n",
    "2. Create a dictionary for all tweets within the sample, and a sub-dictionary that contains the necessary parameters (topic, etc). For instance `tweet_dict = {'twtid':{'topic':'protests','twt_text':'bla bla bla','userid':'123456'}, ...}`\n",
    "3. Iterate through all tweets **not** on the topic, and using the convesation relationships from #1, find the the tweets that each tweet refers to (i.e. the *parent* to the *child*)\n",
    "4. Use the dictionary of tweets and topics from #2, to check which topic the *parent* tweet is on, and if its on the topic, change the topic of the *child*\n",
    "5. Continue until no more tweets are recategorized with each loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvoTopic:\n",
    "    \"\"\"\n",
    "    ConvoTopic changes the topic of tweets not categorized about a topic to the topic by \n",
    "    checking the conversation thread affects.  \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 retweet_list,\n",
    "                 reply_list, \n",
    "                 quote_list):\n",
    "        \"\"\"\n",
    "        @parameters:\n",
    "        - retweet_list - list of retweets in the manner of [('tweet id','retweeted tweet id'),...]\n",
    "        - reply_list - list of all replies in the manner of [('tweet id','replyng to tweet id'),...]\n",
    "        - quote_list - list of all times a quote is made, [('tweet id','quoting tweet id'),...]\n",
    "        \"\"\"\n",
    "        self.replies = {each[0]:each[1] for each in reply_list}\n",
    "        self.quotes = {each[0]:each[1] for each in quote_list}\n",
    "        self.retweets = {each[0]:each[1] for each in retweet_list}\n",
    "\n",
    "    #----------------------main function--------------------------------------------------------------------------------\n",
    "    def Recategorize_one_topic(self, tweet_list, change_topic_label, no_topic_label=\"NA\"):\n",
    "        \"\"\"\n",
    "        Check and recategorize the tweets NOT about the topic but should be\n",
    "        \n",
    "        @parameter:\n",
    "            - tweet_list - list of tweets with tuples, such as [('twtid','topic',...),...]\n",
    "                NOTE the first must be the tweet id, the second the topic label, further inputs are ignored and returned as is\n",
    "            - no_topic_label - what is the label of topics that have no label associated with them. \n",
    "                default - 'NA'\n",
    "        @returns: dict of tweets and their topics \n",
    "        \"\"\"\n",
    "        self.change_topic_label = change_topic_label\n",
    "        #first, just run the regular categorization and categorize based on keyword matches\n",
    "        if len(tweet_list[0]) > 2:\n",
    "            self.initial_tweet_dict = {each[0]:{'topic':each[1],'other_params':[each[2:]]} for each in tweet_list}\n",
    "        else:\n",
    "            self.initial_tweet_dict = {each[0]:{'topic':each[1]} for each in tweet_list}\n",
    "        self.no_topic_label = no_topic_label\n",
    "        return self._recategorize_mast_()\n",
    "        \n",
    "    #----------------------supporting functions--------------------------------------------------------------------------\n",
    "    def _recategorize_mast_(self):\n",
    "        \"\"\"\n",
    "        main function that recagorizes tweets based on the initial input of tweets\n",
    "        \"\"\"\n",
    "        #create a copy of the original tweet dict to work with\n",
    "        tweet_dict = self.initial_tweet_dict.copy()\n",
    "        i = 1\n",
    "        while True:\n",
    "            n_changed = 0\n",
    "            #iterate over all the tweets in the master input dict\n",
    "            for tweet, value in self.initial_tweet_dict:\n",
    "                #only check and change the topic if the topic was NOT on the topic of interest\n",
    "                if value['topic'] == self.no_topic_label:\n",
    "                    result = self._recategorize_check_tweet_(tweet)\n",
    "                    if result == \"change\":\n",
    "                        tweet_dict[tweet]['topic'] == self.change_topic_label\n",
    "                        n_changed += 1\n",
    "            print(\"{i} iteration completed, recategorized this round: {n}\".format(i=i,n=n_changed))\n",
    "            i += 1\n",
    "            #if no more tweets are being changed, exit the loop\n",
    "            if n_changed == 0:\n",
    "                break\n",
    "        return new_tweet_list\n",
    "    \n",
    "    def _recategorize_check_tweet_(self, twtid):\n",
    "        \"\"\"\n",
    "        check the tweet that is inputted as about the applicable rules\n",
    "        \"\"\"\n",
    "        #default 'result' of the check is None. If a match is made with any of the convo links, then 'result' is updated. \n",
    "        result = None\n",
    "        #check replies -- if this tweet was a reply to the other tweet\n",
    "        if twtid in self.replies and self.replies[twtid] in self.initial_tweet_dict:\n",
    "            #check if the topic of the replied to tweet was on the topic we are listening to\n",
    "            if self.initial_tweet_dict[self.replies[twtid]]['topic'] == self.change_topic_label:\n",
    "                result = \"change\"\n",
    "        #check quotes -- if this tweet quoted another tweet\n",
    "        if twtid in self.quotes and self.quotes[twtid] in self.initial_tweet_dict:\n",
    "            #now check if the topic of the quoted tweet was on the topic of interest \n",
    "            if self.initial_tweet_dict[self.quotes[twtid]]['topic'] == self.change_topic_label:\n",
    "                result = \"change\"\n",
    "        #check retweets -- if this tweet retweeted another tweet\n",
    "        if twtid in self.retweets and self.retweets[twtid] in self.initial_tweet_dict:\n",
    "            #if the retweeted tweet was on the topic, then change the retweeting tweet topic\n",
    "            if self.initial_tweet_dict[self.retweets[twtid]]['topic'] == self.change_topic_label:\n",
    "                result = \"change\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! That should be sufficient. Now we need to prepare some data to feed it and test it:\n",
    "1. Categorize some tweets about a topic\n",
    "2. Prepare the `replies`, `retweets`, and `quotes` lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get some data and pre-categorize tweets as about a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to kremlin_tweets_db database\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nlpru import Cleaner\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "from pysqlc import DB\n",
    "db = DB('kremlin_tweets_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE*, for this example, I am using a pre-normalized data saved in a MySQL server on a *database* called *kremlin_tweets_db*. For more information about this topic, please see our paper [*\"Kremlin Tweets: the politics of social media and the quest for legitimacy in Putin’s Russia\"*](https://www.rudatalab.com/analysis/); or about the normalizing methodology, see the [db schema](https://github.com/sergegoussev/Twitter_analysis/tree/master/SQL) and [normalizing script here](https://github.com/sergegoussev/Twitter_analysis/blob/master/Python/_MySQL_CategorizeNSave3.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate the Cleaner from nlpru\n",
    "C = Cleaner()\n",
    "    \n",
    "def __get_data__(start_date, end_date, twt_createdat=False, communities=False):\n",
    "    \"\"\"\n",
    "    Isolate the get data portion into a sepearate query (for easier use later)\n",
    "    \n",
    "    There are 2 options built into the script:\n",
    "        - twt_createdat -- if True, then the time stamp of the tweet will be included\n",
    "        - communities -- if included, then they will be listed for each user (under tweet)\n",
    "    \"\"\"\n",
    "    #first get create the SQL query that will collect the data. \n",
    "    #As we will need the FreqDist, we want the retweeted text as well so we do a UNION\n",
    "    if twt_createdat == False:\n",
    "        twt_createdat_string = \"\"\n",
    "    elif twt_createdat == True:\n",
    "        twt_createdat_string = \", twt_createdat\"\n",
    "    if communities == False:\n",
    "        c_column = \"\"\n",
    "        c_tables = \"\"\n",
    "    elif communities == True:\n",
    "        c_column = \", imrev3\"\n",
    "        c_tables = \"\"\"\n",
    "        \n",
    "    LEFT JOIN meta_all_users_communities com\n",
    "    ON tmast.userid=com.userid\n",
    "        \"\"\"\n",
    "    q = \"\"\"\n",
    "     SELECT \n",
    "        tmast.twttext as twttext,\n",
    "        tsamp.twtid,\n",
    "        tmast.userid{twt_createdat_string}{c_column}\n",
    "    FROM samp_twts_all_rus_twts_str tsamp \n",
    "        INNER JOIN twt_Master tmast \n",
    "        ON tsamp.twtid=tmast.twtid{c_tables} \n",
    "\n",
    "    WHERE tmast.twt_lang='ru'  \n",
    "    AND tmast.twt_createdat >= '{start}'\n",
    "    AND tmast.twt_createdat < '{end}'\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT\n",
    "        tmast.twttext AS twttext,\n",
    "        tsamp.twtid,\n",
    "        trts.userid{twt_createdat_string}{c_column}\n",
    "    FROM samp_twts_all_rus_twts_str tsamp\n",
    "        INNER JOIN twt_rtmaster trts\n",
    "        ON tsamp.twtid=trts.twtid\n",
    "        INNER JOIN twt_master tmast\n",
    "        ON trts.rttwtid=tmast.twtid{c_tables}\n",
    "\n",
    "    WHERE tmast.twt_lang='ru' \n",
    "    AND tmast.twt_createdat >= '{start}'\n",
    "    AND tmast.twt_createdat < '{end}';\n",
    "    \"\"\".format(start=start_date,\n",
    "               end=end_date,\n",
    "               twt_createdat_string=twt_createdat_string,\n",
    "               c_column=c_column,\n",
    "               c_tables=c_tables)\n",
    "    raw = db.query(q)\n",
    "    print(\"There are {:,} tweets in the captured sample!\".format(len(raw)))\n",
    "    return raw\n",
    "\n",
    "def __check_words_in_doc__(document):\n",
    "    \"\"\"\n",
    "    isolate the checking of words from a document into a separate function (for easier use later)\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for word in word_tokenize(document):\n",
    "        result = C.Check_word(word, remove_proper_nouns=False)\n",
    "        if result['status'] == 'ok':\n",
    "            words.append(result['word'])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 44,613 tweets in the captured sample!\n"
     ]
    }
   ],
   "source": [
    "start_date='2017-03-26'\n",
    "end_date='2017-03-27'\n",
    "raw = __get_data__(start_date=start_date, end_date=end_date, twt_createdat=True, communities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add some keywords and classify this data as *about* a topic or *not* based on a match of **at least one** keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets say we pick the following keywords:\n",
    "keywords1 = \"россия, москва, митинг, навальный, задержать, против, акция, полицейский, димонответить, димон, протест, коррупция\"\n",
    "keywords = keywords1.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list_of_tweets = []\n",
    "for tweet in raw:\n",
    "    clean_words = __check_words_in_doc__(tweet[0])\n",
    "    if len(set(keywords).intersection(clean_words)) != 0:\n",
    "        topic = \"Protests\"\n",
    "    else:\n",
    "        topic = \"others\"\n",
    "    final_list_of_tweets.append((tweet[3], topic, tweet[0], clean_words, tweet[1], tweet[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twt_createdat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Protests</th>\n",
       "      <td>15.123395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>84.876605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          twt_createdat\n",
       "Topic                  \n",
       "Protests      15.123395\n",
       "others        84.876605"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(final_list_of_tweets, columns=[\"twt_createdat\",\"Topic\",\"Tweet\",\"Clean words\",\"twtid\",\"userid\"])\n",
    "df[[\"twt_createdat\",\"Topic\"]].groupby(\"Topic\").count()/df[\"twt_createdat\"].count()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so now we have a set of 44,613 tweets, and we know that aprox 15% of them are categorized as being about the topic we picked based on keywords. \n",
    "\n",
    "### 2. Prepare the conversation thread linkages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the *list of replies* for this sample and date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_q = \"\"\"\n",
    "SELECT \n",
    "    repl.twtid, \n",
    "    inreplytotwtid\n",
    "FROM meta_repliesmaster repl\n",
    "    INNER JOIN samp_twts_all_rus_twts_str samp\n",
    "    ON repl.twtid=samp.twtid\n",
    "    \n",
    "    INNER JOIN twt_master tm\n",
    "    ON tm.twtid=repl.twtid\n",
    "    \n",
    "WHERE tm.twt_createdat >= '{start_date}'\n",
    "AND tm.twt_createdat < '{end_date}'\n",
    "\"\"\".format(start_date=start_date, end_date=end_date)\n",
    "reply_list = db.query(repl_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the *retweets* for this sample and date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_q = \"\"\"\n",
    "SELECT \n",
    "    rt.twtid,\n",
    "    rttwtid\n",
    "FROM twt_rtmaster rt\n",
    "    INNER JOIN samp_twts_all_rus_twts_str samp\n",
    "    ON rt.twtid=samp.twtid\n",
    "    \n",
    "WHERE rt.rt_createdat >= '{start_date}'\n",
    "AND rt.rt_createdat < '{end_date}'\n",
    "\"\"\".format(start_date=start_date, end_date=end_date)\n",
    "retweet_list = db.query(retweet_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the *quotes* for this sample and the date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_q = \"\"\"\n",
    "SELECT \n",
    "    qt.twtid,\n",
    "    qttwtid\n",
    "FROM twt_qtmaster qt\n",
    "    INNER JOIN samp_twts_all_rus_twts_str samp\n",
    "    ON qt.twtid=samp.twtid\n",
    "    \n",
    "    INNER JOIN twt_master tm\n",
    "    ON tm.twtid=qt.twtid\n",
    "    \n",
    "WHERE tm.twt_createdat >= '{start_date}'\n",
    "AND tm.twt_createdat < '{end_date}'\n",
    "\"\"\".format(start_date=start_date, end_date=end_date)\n",
    "quote_list = db.query(quote_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Now that all data is assembled, lets try to run the model and see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = ConvoTopic(\n",
    "    retweet_list = retweet_list,\n",
    "    reply_list = reply_list,\n",
    "    quote_list = quote_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26034"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T.Recategorize_one_topic(\n",
    "#     tweet_list=final_list_of_tweets,\n",
    "#     change_topic_label = \"Protests\",\n",
    "#     no_topic_label = \"others\"\n",
    "# )\n",
    "initial_tweet_dict = {each[0]:{'topic':each[1],'other_params':[each[2:]]} for each in final_list_of_tweets}\n",
    "len(final_list_of_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
